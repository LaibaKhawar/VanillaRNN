{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (2.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: numpy in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\laiba\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Load Shakespeare dataset\n",
    "dataset = load_dataset(\"tiny_shakespeare\", trust_remote_code=True)\n",
    "\n",
    "# Convert text to lowercase and split into words\n",
    "text = dataset['train']['text'][0].lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "word_counts = Counter(text)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Convert words to indices\n",
    "indexed_text = [word2idx[word] for word in text]\n",
    "\n",
    "# Create training sequences (e.g., sequence length = 5)\n",
    "SEQ_LEN = 5\n",
    "X, Y = [], []\n",
    "for i in range(len(indexed_text) - SEQ_LEN):\n",
    "    X.append(indexed_text[i:i+SEQ_LEN])\n",
    "    Y.append(indexed_text[i+SEQ_LEN])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "# Train-test split (80-20)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "Y_train, Y_test = Y[:train_size], Y[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = Y_train.max().item() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = Y_train.max().item() + 1\n",
    "# print(f\"Unique token indices: {sorted(set(Y_train.tolist()))[-10:]}\")  # Last 10 indices\n",
    "# print(f\"Max index in dataset: {Y_train.max().item()}, VOCAB_SIZE: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Max index in Y_train: {Y_train.max().item()}, Min index: {Y_train.min().item()}, VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "# Y_train = torch.clamp(Y_train, min=0, max=VOCAB_SIZE - 1).long()\n",
    "\n",
    "# assert Y_train.max().item() < VOCAB_SIZE, f\"Error: Y_train contains {Y_train.max().item()} which is out of range!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Max index in Y_train: {Y_train.max().item()}, Min index: {Y_train.min().item()}, VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "# # Y_train = toch.clamp(Y_train, min=0, max=VOCAB_SIZE - 1).long()\n",
    "\n",
    "# assert Y_train.max().item() < VOCAB_SIZE, f\"Error: Y_train contains {Y_train.max().item()} which is out of range!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid_indices = batch_Y[batch_Y >= VOCAB_SIZE]\n",
    "# if len(invalid_indices) > 0:\n",
    "#     print(f\"Invalid indices: {invalid_indices}\")\n",
    "# batch_Y = torch.clamp(batch_Y, min=0, max=VOCAB_SIZE - 1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_Y = torch.clamp(batch_Y, min=0, max=VOCAB_SIZE - 1).long() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Max target index: {batch_Y.max().item()}\")\n",
    "# print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "# assert batch_Y.max().item() < VOCAB_SIZE, f\"Index {batch_Y.max().item()} is out of bounds! Max index is {VOCAB_SIZE - 1}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_X = torch.clamp(batch_X, min=0, max=VOCAB_SIZE - 1)\n",
    "# print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "# print(f\"Max index in batch_X: {batch_X.max().item()}\")\n",
    "# print(f\"Min index in batch_X: {batch_X.min().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Max index in batch_X: {batch_X.max().item()}\")\n",
    "# print(f\"Min index in batch_X: {batch_X.min().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the RNN model\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Get last time-step output\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "VOCAB_SIZE = min(10000, len(vocab))  # Limit vocab size to 10,000 words\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LR = 0.001\n",
    "train_losses = []\n",
    "\n",
    "# Ensure tensors are in correct data types\n",
    "X_train = X_train.long()  # Ensure it's integer-based (word indices)\n",
    "Y_train = Y_train.long()  # Ensure labels are class indices\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "SEQ_LENGTH = 5  # Number of words to use as context\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, indexed_text, seq_length):\n",
    "        self.data = []\n",
    "        for i in range(len(indexed_text) - seq_length):\n",
    "            x = torch.tensor(indexed_text[i:i+seq_length])\n",
    "            y = torch.tensor(indexed_text[i+seq_length])\n",
    "            # Clamp values to ensure they are within valid range\n",
    "            x = torch.clamp(x, min=0, max=VOCAB_SIZE - 1)\n",
    "            y = torch.clamp(y, min=0, max=VOCAB_SIZE - 1)\n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "dataset = ShakespeareDataset(indexed_text, SEQ_LENGTH)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = VanillaRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "random_embeddings = model.embedding.weight.data.clone().cpu().numpy()\n",
    "\n",
    "# Training loop with batch processing\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "learned_embeddings = model.embedding.weight.data.clone().cpu().numpy()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"shakespeare_rnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `vocab` is a set of unique words in your dataset\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "VOCAB_SIZE = len(word_to_idx)  # Update vocab size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(seed_text, model, word_to_idx, idx_to_word, num_words=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = seed_text.lower().split()\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        input_seq = [word_to_idx.get(word, 0) for word in words[-SEQ_LENGTH:]]\n",
    "        input_tensor = torch.tensor([input_seq])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            scaled_logits = output / temperature\n",
    "            probabilities = F.softmax(scaled_logits, dim=1)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_word_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            next_word = idx_to_word[next_word_idx]\n",
    "\n",
    "        words.append(next_word)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Try generating with a moderate temperature (0.7-1.2)\n",
    "generated_sentence = generate_text(\"to be or not to\", model, word_to_idx, idx_to_word, num_words=10, temperature=0.8)\n",
    "print(\"\\nGenerated Text:\", generated_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in loader:\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_Y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity\n",
    "\n",
    "train_perplexity = compute_perplexity(train_loader, model)\n",
    "test_perplexity = compute_perplexity(test_loader, model)\n",
    "\n",
    "print(f\"\\nTrain Perplexity: {train_perplexity:.2f}\")\n",
    "print(f\"Test Perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_accuracy(model, test_loader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            output = model(batch_X)  # Get model predictions\n",
    "            predicted_indices = output.argmax(dim=1)  # Get predicted word index\n",
    "            correct += (predicted_indices == batch_Y).sum().item()  # Count correct predictions\n",
    "            total += batch_Y.size(0)  # Total words processed\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Compute and print word-level accuracy\n",
    "word_accuracy = compute_word_accuracy(model, test_loader)\n",
    "print(f\"Word-Level Accuracy: {word_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Ensure model and data are properly defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model and data to GPU\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "model = model.to(device)\n",
    "model.train()  # ✅ Ensure training mode\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE = 5000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Ensure input values are within range\n",
    "X_train = torch.clamp(X_train, min=0, max=VOCAB_SIZE - 1)\n",
    "Y_train = torch.clamp(Y_train, min=0, max=VOCAB_SIZE - 1)\n",
    "\n",
    "# Define DataLoader\n",
    "dataset = TensorDataset(X_train, Y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Enable gradient checkpointing if available\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loss tracking\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()  # ✅ Ensure training mode each epoch\n",
    "\n",
    "    for X_batch, Y_batch in dataloader:\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Mixed precision\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Set to evaluation mode for inference\n",
    "model.eval()\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(range(1, EPOCHS + 1), losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare learned embeddings with randomly initialized ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Compute cosine similarity between random and learned embeddings\n",
    "similarity = np.mean(cosine_similarity(random_embeddings, learned_embeddings))\n",
    "print(f\"Average Cosine Similarity between Random & Learned Embeddings: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define device (use GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "model.to(device)\n",
    "\n",
    "# Function to calculate word-level accuracy\n",
    "def calculate_word_level_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in data_loader:\n",
    "            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)  # Move to correct device\n",
    "            output = model(batch_X)  # Forward pass\n",
    "            predictions = torch.argmax(output, dim=1)  # Get predicted word index\n",
    "            correct += (predictions == batch_Y).sum().item()  # Compare predictions\n",
    "            total += batch_Y.size(0)\n",
    "\n",
    "    return correct / total if total > 0 else 0  # Accuracy as a fraction\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(model, data_loader):\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in data_loader:\n",
    "            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)  # Move to correct device\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_Y)\n",
    "            total_loss += loss.item() * batch_Y.size(0)\n",
    "            total_words += batch_Y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_words if total_words > 0 else float('inf')\n",
    "    return np.exp(avg_loss)  # Perplexity = exp(average loss)\n",
    "\n",
    "# Convert embeddings to torch tensors and move them to the correct device\n",
    "random_embeddings_tensor = torch.tensor(random_embeddings, device=device)\n",
    "learned_embeddings_tensor = torch.tensor(learned_embeddings, device=device)\n",
    "\n",
    "# Copy random embeddings to model\n",
    "model.embedding.weight.data.copy_(random_embeddings_tensor)\n",
    "\n",
    "# Calculate accuracy and perplexity for random embeddings\n",
    "random_accuracy = calculate_word_level_accuracy(model, test_loader)\n",
    "random_perplexity = calculate_perplexity(model, test_loader)\n",
    "\n",
    "# Copy learned embeddings to model\n",
    "model.embedding.weight.data.copy_(learned_embeddings_tensor)\n",
    "\n",
    "# Calculate accuracy and perplexity for learned embeddings\n",
    "learned_accuracy = calculate_word_level_accuracy(model, test_loader)\n",
    "learned_perplexity = calculate_perplexity(model, test_loader)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Random Embeddings - Word-Level Accuracy: {random_accuracy:.4f}, Perplexity: {random_perplexity:.4f}\")\n",
    "print(f\"Learned Embeddings - Word-Level Accuracy: {learned_accuracy:.4f}, Perplexity: {learned_perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN model using pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio gensim numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# =====================\n",
    "# 1. Load Shakespeare Dataset\n",
    "# =====================\n",
    "dataset = load_dataset(\"tiny_shakespeare\")\n",
    "train_text = dataset['train']['text']\n",
    "train_text = \" \".join(train_text)  # Convert list of sentences into one large text corpus\n",
    "\n",
    "# Tokenize text into words\n",
    "words = train_text.lower().split()\n",
    "vocab = {word: idx for idx, word in enumerate(set(words))}\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "# Convert words to indices\n",
    "indexed_text = [vocab[word] for word in words]\n",
    "\n",
    "# =====================\n",
    "# 2. Prepare Sequences for Training\n",
    "# =====================\n",
    "SEQ_LENGTH = 5  # Number of previous words used to predict next word\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, indexed_text, seq_length):\n",
    "        self.data = []\n",
    "        for i in range(len(indexed_text) - seq_length):\n",
    "            self.data.append((indexed_text[i:i+seq_length], indexed_text[i+seq_length]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "dataset = ShakespeareDataset(indexed_text, SEQ_LENGTH)\n",
    "\n",
    "# Split dataset into train & validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# =====================\n",
    "# 3. Load Pretrained Word2Vec Embeddings\n",
    "# =====================\n",
    "word2vec = api.load(\"word2vec-google-news-300\")  # 300-dim embeddings\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 10\n",
    "LR = 0.0025\n",
    "\n",
    "# Prepare pretrained embeddings matrix\n",
    "embedding_matrix = np.random.uniform(-0.1, 0.1, (VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, idx in vocab.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "\n",
    "# =====================\n",
    "# 4. Define the RNN Model\n",
    "# =====================\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pretrained_embeddings=None):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Get last time-step output\n",
    "        return out\n",
    "\n",
    "# =====================\n",
    "# 5. Train & Evaluate Models\n",
    "# =====================\n",
    "models = {\n",
    "    \"Random Embeddings\": VanillaRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM),\n",
    "    \"Pretrained Word2Vec\": VanillaRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, pretrained_embeddings=embedding_matrix)\n",
    "}\n",
    "\n",
    "# Loss Function & Optimizer\n",
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_X, batch_Y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation Loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_Y in val_loader:\n",
    "                output = model(batch_X)\n",
    "                val_loss += criterion(output, batch_Y).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train both models & Store Loss Curves\n",
    "loss_curves = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    loss_curves[name] = train_model(model, train_loader, val_loader)\n",
    "\n",
    "# =====================\n",
    "# 6. Evaluate Model Performance\n",
    "# =====================\n",
    "# def evaluate_model(model, val_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     loss = 0\n",
    "#     perplexity = 0\n",
    "#     all_preds, all_labels = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_X, batch_Y in val_loader:\n",
    "#             output = model(batch_X)\n",
    "#             loss += F.cross_entropy(output, batch_Y).item()\n",
    "#             preds = torch.argmax(output, dim=1)\n",
    "\n",
    "#             # Calculate Accuracy\n",
    "#             correct += (preds == batch_Y).sum().item()\n",
    "#             total += batch_Y.size(0)\n",
    "\n",
    "#             # Store predictions for confusion matrix\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(batch_Y.cpu().numpy())\n",
    "\n",
    "#             # Perplexity Calculation\n",
    "#             log_probs = F.log_softmax(output, dim=1)\n",
    "#             perplexity += torch.exp(-log_probs.gather(1, batch_Y.view(-1, 1)).mean()).item()\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     avg_loss = loss / len(val_loader)\n",
    "#     avg_perplexity = perplexity / len(val_loader)\n",
    "\n",
    "#     return avg_loss, accuracy, avg_perplexity, all_preds, all_labels\n",
    "\n",
    "# # Compute evaluation metrics\n",
    "# metrics = {}\n",
    "# for name, model in models.items():\n",
    "#     print(f\"\\nEvaluating {name}...\")\n",
    "#     metrics[name] = evaluate_model(model, val_loader)\n",
    "\n",
    "# # =====================\n",
    "# # 7. Plot Loss Curve\n",
    "# # =====================\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for name, losses in loss_curves.items():\n",
    "#     plt.plot(losses, label=name)\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training Loss Curve\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # =====================\n",
    "# # 8. Confusion Matrix\n",
    "# # =====================\n",
    "# for name, (loss, accuracy, perplexity, preds, labels) in metrics.items():\n",
    "#     print(f\"\\n{name} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "#     cm = confusion_matrix(labels, preds)\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(vocab.keys())[:10], yticklabels=list(vocab.keys())[:10])\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"Actual\")\n",
    "#     plt.title(f\"Confusion Matrix for {name}\")\n",
    "#     plt.show()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate_model(model, val_loader):\n",
    "    loss_curves = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in val_loader:\n",
    "            output = model(batch_X)  # Forward pass\n",
    "            loss = criterion(output, batch_Y)  # Compute loss\n",
    "            total_loss += loss.item() * batch_Y.size(0)\n",
    "            total_words += batch_Y.size(0)\n",
    "\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            labels.extend(batch_Y.cpu().numpy())\n",
    "\n",
    "            total_correct += (preds == batch_Y).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_words\n",
    "    accuracy = total_correct / total_words\n",
    "    perplexity = np.exp(avg_loss) if avg_loss < 10 else float('inf')  # Prevent overflow\n",
    "\n",
    "    return avg_loss, accuracy, perplexity, predictions, labels\n",
    "\n",
    "# Evaluate multiple models\n",
    "metrics = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    metrics[name] = evaluate_model(model, val_loader)\n",
    "\n",
    "# =====================\n",
    "# 7. Plot Loss Curve\n",
    "# =====================\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name, losses in loss_curves.items():\n",
    "    plt.plot(losses, label=name)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# =====================\n",
    "# 8. Confusion Matrix\n",
    "# =====================\n",
    "for name, (loss, accuracy, perplexity, preds, labels) in metrics.items():\n",
    "    print(f\"\\n{name} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "    # Ensure predictions and labels are flattened\n",
    "    preds = np.array(preds).flatten()\n",
    "    labels = np.array(labels).flatten()\n",
    "\n",
    "    # Limit confusion matrix size for memory efficiency (top 10 labels)\n",
    "    top_n = 10\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm[:top_n, :top_n], annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=list(vocab.keys())[:top_n],\n",
    "        yticklabels=list(vocab.keys())[:top_n]\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix for {name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Clean up memory after each evaluation\n",
    "    del preds, labels\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences generated from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def generate_text(model, start_text, vocab, reverse_vocab, max_length=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = start_text.lower().split()\n",
    "    input_seq = [vocab[word] for word in words if word in vocab]\n",
    "    generated_words = words[:]  # Start with the given words\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if len(input_seq) < SEQ_LENGTH:\n",
    "                padded_seq = [0] * (SEQ_LENGTH - len(input_seq)) + input_seq  # Pad with zeros if needed\n",
    "            else:\n",
    "                padded_seq = input_seq[-SEQ_LENGTH:]  # Take last SEQ_LENGTH words\n",
    "\n",
    "            input_tensor = torch.tensor([padded_seq]).to(torch.int64)  # Convert to tensor\n",
    "            output = model(input_tensor).squeeze(0)  # Get raw logits\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            scaled_logits = output / temperature\n",
    "            probabilities = F.softmax(scaled_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Sample next word based on probability distribution\n",
    "            predicted_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "            # Convert index to word\n",
    "            if predicted_index in reverse_vocab:\n",
    "                next_word = reverse_vocab[predicted_index]\n",
    "                generated_words.append(next_word)\n",
    "                input_seq.append(predicted_index)\n",
    "            else:\n",
    "                break  # Stop if the word is not in vocab\n",
    "\n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "# Reverse vocabulary (index → word mapping)\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Generate text using the trained model with Word2Vec and temperature\n",
    "start_text = \"to be\"\n",
    "generated_text_word2vec = generate_text(models[\"Pretrained Word2Vec\"], start_text, vocab, reverse_vocab, temperature=0.8)\n",
    "print(\"Generated Text (Word2Vec):\", generated_text_word2vec)\n",
    "\n",
    "# Generate text using the randomly initialized embeddings model with temperature\n",
    "generated_text_random = generate_text(models[\"Random Embeddings\"], start_text, vocab, reverse_vocab, temperature=0.8)\n",
    "print(\"Generated Text (Random Embeddings):\", generated_text_random)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
